{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":34877,"sourceType":"datasetVersion","datasetId":27352}],"dockerImageVersionId":30839,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:36:37.294453Z","iopub.execute_input":"2025-01-20T04:36:37.294832Z","iopub.status.idle":"2025-01-20T04:36:37.302284Z","shell.execute_reply.started":"2025-01-20T04:36:37.294801Z","shell.execute_reply":"2025-01-20T04:36:37.300772Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/mnist-in-csv/mnist_test.csv\n/kaggle/input/mnist-in-csv/mnist_train.csv\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"import pandas as ds # Pandas is a libaray that used to work with datasets\nimport numpy as np # Manipulating arrays\nfrom matplotlib import pyplot as plt #Pyplot exists as a module inside the mathplotlib (not a standalone package)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:36:40.929432Z","iopub.execute_input":"2025-01-20T04:36:40.929815Z","iopub.status.idle":"2025-01-20T04:36:40.934143Z","shell.execute_reply.started":"2025-01-20T04:36:40.929781Z","shell.execute_reply":"2025-01-20T04:36:40.932950Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"#using the training data, train our neural network\n\ndata = pd.read_csv('/kaggle/input/mnist-in-csv/mnist_train.csv')\n\ndata = np.array(data) #covert pandas dataframe into ndarray- now a 2D array (60,000, 784)\n\n\n\n#Assign X and Y \nY = data[1:10000, 0:1]\nX = data[1:10000,1:]\nprint (Y.shape)\nprint(X.shape)\nm, n = X.shape\nprint(m,n)\n\n#we want to transpose both of them to be able to work in our calculations\nY = Y.T\nX = X.T\n\nX = X / 255.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:42:45.110607Z","iopub.execute_input":"2025-01-20T04:42:45.111019Z","iopub.status.idle":"2025-01-20T04:42:48.374577Z","shell.execute_reply.started":"2025-01-20T04:42:45.110975Z","shell.execute_reply":"2025-01-20T04:42:48.373460Z"}},"outputs":[{"name":"stdout","text":"(9999, 1)\n(9999, 784)\n9999 784\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"#initalize our weights, and bias point as matrix\n# Input layer: 784, Hidden layer: 10, Output layer: 0-9\n\n'''\nWhat should be the dimentions of the matrix?\n\n1. Weights \n    a. Input -> Hidden layer (784, 10) \n    b. Hidden -> Output (10, 10)\n\n2. Bias\n    a. Hidden layer (10, 1)\n    b. Output layer (10, 1)\n'''\n# randn: produce random numbers between [0,1]\n#subtract 0.5 to generate negative values as well [-0.5,0.5]\ndef initalize_parameters():\n    W1 = np.random.rand(10,784) - 0.5 # each row corresponds to the node's weight respect to the other 784 input layers\n    W2 = np.random.rand (10,10) - 0.5\n    B1 = np.random.rand (10,1) - 0.5\n    B2 = np.random.rand (10,1) - 0.5\n    return W1, W2, B1, B2\n\n#we want to use an activation function so that our output isn't just a linear combination of the input\ndef activation_ReLU(Z):\n    return np.maximum(Z,0)\n\n# Since we are using ReLU activation (can generate numbers above 1), we need output between 0-1\n# we also have to ensure that they are the same relative to eachother\ndef softmax(Z):\n    A = np.exp(Z) / sum(np.exp(Z))\n    return A\n\n\ndef forward_propgation(W1,W2,B1,B2,X): #where X is the 784 values of pixel image\n    Z1 = W1.dot(X) + B1 #let the hidden layer values W1: (10, 784) X: (784, 1) -> dimentions of (10,1)\n    A1 = activation_ReLU(Z1)\n\n    Z2 = W2.dot(A1) + B2 #Output later W2: (10,10) A1: (10,1)-> Z2: (10,1)\n    A2 = softMax(Z2)\n    return Z1, A1, Z2, A2\n\n#one hot encoding: a way to tell your program which output should be right\ndef one_hot_encode(Y): #Y is your answer vector in dimentions (Y.size, 1): row, coloumn\n    #turn your answer vector into a matrix\n    matrix = np.zeros((Y.size, Y.max() + 1))\n    matrix[np.arange(Y.size), Y] = 1 #arrange creates a array from 0 - Y.size-1, so then u can use to index\n    return matrix.T #matrix is now in dimentions (Y.size(), 10): row, coloumn\n\n# we use backward propogation to change the weights and biases, accoriding to input and expected output data\ndef backward_propogation(Z1, A1, Z2, A2, W2, Y):\n    output_encoded = one_hot_encode(Y)\n    #keep track of all the small changes\n    dZ2 = A2 - output_encoded #the difference between expected and actual values\n    dW2 = 1 / m * dZ2.dot(A1.T)\n    dB2 = 1 / m * np.sum(dZ2)\n    dZ1 = W2.T.dot(dZ2) * activation_ReLU(Z1)\n    dW1 = 1 / m * dZ1.dot(X.T)\n    dB1 = 1 / m * np.sum(dZ1)\n    return dW1, dB1, dW2, dB2\n\ndef learn(W1, B1, W2, B2, dW1, dB1, dW2, dB2, alpha):\n    #we want to update the biases and weights, by the learning rate: alpha\n    W1 = W1 - alpha * dW1\n    B1 = B1 - alpha * dB1    \n    W2 = W2 - alpha * dW2  \n    B2 = B2 - alpha * dB2    \n    return W1, B1, W2, B2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:40:27.242873Z","iopub.execute_input":"2025-01-20T04:40:27.243260Z","iopub.status.idle":"2025-01-20T04:40:27.253201Z","shell.execute_reply.started":"2025-01-20T04:40:27.243228Z","shell.execute_reply":"2025-01-20T04:40:27.251928Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"#this will get the largest value from the output (what the neural network thinks the number is)\ndef get_predictions(A2):\n    return np.argmax(A2, 0)\n\ndef get_accuracy(predictions, Y):\n    print(predictions, Y) #print the expected and actual\n    return np.sum(predictions == Y) / Y.size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:36:52.239854Z","iopub.execute_input":"2025-01-20T04:36:52.240183Z","iopub.status.idle":"2025-01-20T04:36:52.245256Z","shell.execute_reply.started":"2025-01-20T04:36:52.240158Z","shell.execute_reply":"2025-01-20T04:36:52.244129Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"def gradient_descent(X,Y,alpha, iterations):\n    #start with an inital value for weights and biases\n    W1, W2, B1, B2 = initalize_parameters()\n\n    for i in range (iterations):\n        #we want to first forward propogate\n        Z1, A1, Z2, A2= forward_propgation(W1,W2,B1,B2,X) \n        #back propogation to update out weights and biases\n        dW1, dB1, dW2, dB2 = backward_propogation(Z1, A1, Z2, A2, W2, Y)\n        #depending on the error - change the value of weights and biases\n        W1, b1, W2, b2= learn(W1, B1, W2, B2, dW1, dB1, dW2, dB2, alpha)\n        #we only want to print every 100 iterations\n        if i%100 == 0:\n            print(\"Iteration: \", i)\n            predictions = get_predictions(A2)\n            print(get_accuracy(predictions, Y))\n    return W1, B1, W2, B2\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:41:45.290630Z","iopub.execute_input":"2025-01-20T04:41:45.291045Z","iopub.status.idle":"2025-01-20T04:41:45.297679Z","shell.execute_reply.started":"2025-01-20T04:41:45.291012Z","shell.execute_reply":"2025-01-20T04:41:45.296419Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"W1, b1, W2, b2 = gradient_descent(X, Y, 0.10, 10000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:43:27.529356Z","iopub.execute_input":"2025-01-20T04:43:27.529774Z","iopub.status.idle":"2025-01-20T04:46:27.419522Z","shell.execute_reply.started":"2025-01-20T04:43:27.529741Z","shell.execute_reply":"2025-01-20T04:46:27.418127Z"}},"outputs":[{"name":"stdout","text":"Iteration:  0\n[0 0 7 ... 0 7 9] [[0 4 1 ... 6 9 7]]\n0.059905990599059905\nIteration:  100\n[0 6 2 ... 2 1 3] [[0 4 1 ... 6 9 7]]\n0.3694369436943694\nIteration:  200\n[0 6 1 ... 6 8 7] [[0 4 1 ... 6 9 7]]\n0.5457545754575458\nIteration:  300\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.7541754175417542\nIteration:  400\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.8085808580858086\nIteration:  500\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.8323832383238324\nIteration:  600\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.8435843584358436\nIteration:  700\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.8538853885388539\nIteration:  800\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.8613861386138614\nIteration:  900\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.8665866586658666\nIteration:  1000\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.8698869886988699\nIteration:  1100\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.8722872287228722\nIteration:  1200\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.875087508750875\nIteration:  1300\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.8796879687968797\nIteration:  1400\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.8805880588058805\nIteration:  1500\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.8825882588258825\nIteration:  1600\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.885988598859886\nIteration:  1700\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.8890889088908891\nIteration:  1800\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.8923892389238924\nIteration:  1900\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.8958895889588959\nIteration:  2000\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.8984898489848985\nIteration:  2100\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9024902490249025\nIteration:  2200\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.905990599059906\nIteration:  2300\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9075907590759076\nIteration:  2400\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9115911591159116\nIteration:  2500\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9133913391339133\nIteration:  2600\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9145914591459146\nIteration:  2700\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9160916091609161\nIteration:  2800\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9181918191819182\nIteration:  2900\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9210921092109211\nIteration:  3000\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9226922692269227\nIteration:  3100\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9237923792379238\nIteration:  3200\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9248924892489249\nIteration:  3300\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9242924292429243\nIteration:  3400\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9247924792479248\nIteration:  3500\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9260926092609261\nIteration:  3600\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9258925892589259\nIteration:  3700\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9265926592659266\nIteration:  3800\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9270927092709271\nIteration:  3900\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9274927492749275\nIteration:  4000\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9277927792779278\nIteration:  4100\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9287928792879288\nIteration:  4200\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9293929392939294\nIteration:  4300\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9302930293029303\nIteration:  4400\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9306930693069307\nIteration:  4500\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9305930593059306\nIteration:  4600\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9312931293129313\nIteration:  4700\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9323932393239324\nIteration:  4800\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9331933193319332\nIteration:  4900\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9334933493349334\nIteration:  5000\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.934993499349935\nIteration:  5100\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9352935293529353\nIteration:  5200\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9357935793579358\nIteration:  5300\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9358935893589359\nIteration:  5400\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9362936293629363\nIteration:  5500\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9372937293729373\nIteration:  5600\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9373937393739374\nIteration:  5700\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.937993799379938\nIteration:  5800\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9384938493849385\nIteration:  5900\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9390939093909391\nIteration:  6000\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.938993899389939\nIteration:  6100\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9391939193919392\nIteration:  6200\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9400940094009401\nIteration:  6300\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9409940994099409\nIteration:  6400\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9411941194119412\nIteration:  6500\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9416941694169417\nIteration:  6600\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9417941794179417\nIteration:  6700\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9417941794179417\nIteration:  6800\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9416941694169417\nIteration:  6900\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9424942494249425\nIteration:  7000\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9425942594259425\nIteration:  7100\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.942994299429943\nIteration:  7200\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9431943194319432\nIteration:  7300\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9436943694369437\nIteration:  7400\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9436943694369437\nIteration:  7500\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9442944294429443\nIteration:  7600\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9441944194419442\nIteration:  7700\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9442944294429443\nIteration:  7800\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9452945294529453\nIteration:  7900\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9454945494549455\nIteration:  8000\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.945994599459946\nIteration:  8100\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9464946494649465\nIteration:  8200\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9466946694669467\nIteration:  8300\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9470947094709471\nIteration:  8400\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9472947294729473\nIteration:  8500\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9478947894789479\nIteration:  8600\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9481948194819482\nIteration:  8700\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.947994799479948\nIteration:  8800\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9481948194819482\nIteration:  8900\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9484948494849484\nIteration:  9000\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9484948494849484\nIteration:  9100\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9482948294829483\nIteration:  9200\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9486948694869487\nIteration:  9300\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9488948894889488\nIteration:  9400\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9494949494949495\nIteration:  9500\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9495949594959496\nIteration:  9600\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.94999499949995\nIteration:  9700\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9505950595059506\nIteration:  9800\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9501950195019502\nIteration:  9900\n[0 4 1 ... 6 9 7] [[0 4 1 ... 6 9 7]]\n0.9506950695069507\n","output_type":"stream"}],"execution_count":67}]}